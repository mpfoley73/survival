[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Survival Analysis",
    "section": "",
    "text": "Preface\nThese notes are a personal reference related to my survival analysis work.\nRepo for this quarto book: https://github.com/mpfoley73/survival.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-concepts.html",
    "href": "01-concepts.html",
    "title": "1  Concepts",
    "section": "",
    "text": "1.1 Terminology\nSurvival analyses model time-to-event. They differ from linear regression in two respects. Event times are typically skewed right with many early events and few late ones, violating linear regression’s normality assumption. Survival analyses must also manage censoring, an unknown starting event (left censoring) and/or ending event (right censoring)1. Censoring occurs if the event does not take place by the end of the study window, or the subject is in some way lost to follow-up. In Figure 1.1, subjects 3, 5, 9, and 15 either did not have the event or dropped out of the study. Censored observations do not reveal their total time to event, but they do reveal at least their minimum.\nSome analyses treat time-to-event as a continuous outcome with survival times following a parametric distribution such as the Weibull (Chapter 3). A more common approach is the semi-parametric model, with an unspecified time-to-event distribution estimated by non-parametric methods coupled with covariate effects following a parametric distribution. The Cox proportional hazards model is a semi-parametric model (Chapter 4). A third approach treats time-to-event as a series of person-periods with binary outcomes that follow a logit or cumulative log-log distribution. These models are called discrete-time models (Chapter 5). A fourth approach employs machine learning models such as trees and SVMs to optimize predictive power at some expense of interpretability (Chapter 6).\nAny of these approaches can produce accurate and efficient results (Suresh 2022), but the typical survival analysis uses Kaplan-Meier plots to visualize survival curves (Chapter 2), log-rank tests to compare survival curves among groups, and Cox proportional hazards regression to describe the effect of explanatory variables on survival. In R, use the survival package to model, survminer to visualize, and gtsummary for summarize.\nLet \\(T^*\\) be a random variable representing the time until the event, and \\(U\\) be a random variable representing the time until (right) censoring. The observed value is whichever event comes first, \\(T = \\mathrm{min}(T^*, U)\\). The time, \\(T\\), and status, \\(\\delta = I[T^* &lt; U]\\) (1 = censored, 2 = event), constitutes the response in a survival analysis. The survival::lung dataset is typical, where time = \\(T\\) and status = \\(\\delta\\), and the other variables are identifying features for the subject.\nhead(survival::lung)\n\n  inst time status age sex ph.ecog ph.karno pat.karno meal.cal wt.loss\n1    3  306      2  74   1       1       90       100     1175      NA\n2    3  455      2  68   1       0       90        90     1225      15\n3    3 1010      1  56   1       0       90        90       NA      15\n4    5  210      2  57   1       1       90        60     1150      11\n5    1  883      2  60   1       0      100        90       NA       0\n6   12 1022      1  74   1       1       50        80      513       0\nCensoring sometimes occurs when subjects are monitored for a fixed period of time (Type I), the study is halted after a pre-specified level of events are reached (Type II), or the subject drops out for a reason other than the event of interest (random censoring).\nYou can specify the survival distribution either with a survival function, \\(S(t)\\), or with a hazard function, \\(h(t)\\)2. Let \\(F(t) = P(T \\le t)\\) be the cumulative risk function (aka, cumulative incidence), the probability of the event occurring on or before time \\(t\\). \\(S(t)\\) is its complement, \\(S(t) = 1 - F(t)\\).\n\\[S(t) = P(T &gt; t).\\]\nThe hazard function is the instantaneous event rate at \\(t\\) given survival up to \\(t\\),\n\\[h(t) = \\lim_{\\delta \\rightarrow 0}{\\frac{P(t &lt; T &lt; t + \\delta|T &gt; t)}{\\delta}}.\\]\nAn instantaneous event rate has no intuitive appeal, but think of it in discrete time where \\(\\delta &gt; 0\\). \\(h(t + \\delta)\\) is the conditional probability of an event at the discrete interval \\(t + \\delta\\), conditioned on those at risk during that interval.\nThe survival and hazard functions are related by the multiplication rule, \\(P(AB) = P(A|B)P(B)\\). The event probability at \\(t\\), \\(f(t) = F'(t)\\), is the probability of the event at \\(t\\) given survival up to \\(t\\) (the hazard function) multiplied by the probability of survival up to \\(t\\) (the survival function).\n\\[f(t) = h(t) S(t).\\]\nAgain in discrete terms, survival up to interval \\(t\\) is the sum product of the survival probabilities at each preceding period, \\(S(t) = \\Pi_{i = 1}^t [1 - h(t)]\\). It is the complement of the cumulative risk.\nRearranging, \\(h(t)dt = \\frac{f(t)}{S(t)}dt\\) describes the prognosis for a subject who has survived through time \\(t\\).\n\\(S(t)\\) is also the negative exponent of the cumulative hazard function,\n\\[S(t) = e^{-H(t)}.\\]\nTaking the log and rearranging, \\(h(t) = \\frac{d}{dt} [\\log S(t)]\\), the negative time derivative of the log cumulative hazard.\nUse the survival function to estimate the mean survival time, \\(E(T) = \\int S(t)dt\\), and median survival time, \\(S(t) = 0.5\\).\nTake the exponential distribution as a quick example. It has a constant hazard, \\(h(t) = \\lambda\\). The cumulative hazard is \\(H(t) = \\int_0^t \\lambda du = \\lambda t\\). The survival function is \\(S(t) = e^{-\\lambda t}\\). The probability of failure at time \\(t\\) is \\(f(t) = \\lambda e^{-\\lambda t}\\). The expected time to failure is \\(E(t) = \\int_0^\\infty e^{-\\lambda t} dt = 1 / \\lambda\\), and the median time to failure is \\(S(t) = e^{-\\lambda t} = .5\\), or \\(t_{med} = \\log(2) / \\lambda\\).\nThere are parametric and non-parametric methods to estimate a survival curve. The usual non-parametric method is the Kaplan-Meier estimator. The usual parametric method is the Weibull distribution. In between is the most common way to estimate a survivor curve, the Cox proportional hazards model.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts</span>"
    ]
  },
  {
    "objectID": "01-concepts.html#logrank",
    "href": "01-concepts.html#logrank",
    "title": "1  Concepts",
    "section": "1.2 Log-Rank Test",
    "text": "1.2 Log-Rank Test\nIt is not obvious how to compare two survival distributions because they can cross, diverge, etc. When observations do not follow a parametric distribution function, compare them with the non-parametric log-rank test. The alternative hypothesis, termed the Lehmann alternative, is that one survival distribution is uniformly higher than the other, \\(H_A : S_1(t) = [S_0(t)]^\\psi\\), or equivalently, the hazard functions are proportional, \\(h_1(t) = \\psi h_0(t)\\), with \\(H_A: \\psi \\ne 1\\).\nAt each \\(t\\), you could construct a 2x2 contingency table between event/no-event and curves A and B.\n\n\n\n\nCurve A\nCurve B\nTotal\n\n\n\n\nEvent\n\\(d_{Ai}\\)\n\\(d_{Bi}\\)\n\\(d_i\\)\n\n\nNo Event\n\\(n_{Ai} - d_{0i}\\)\n\\(n_{Bi} - d_{1i}\\)\n\\(n_i - d_i\\)\n\n\nTotal\n\\(n_{Ai}\\)\n\\(n_{Bi}\\)\n\\(n_i\\)\n\n\n\nHolding the margins as fixed, the probability of observing \\(d_{Ai}\\) events in curve A at time \\(i\\) follows a hypergeometric distribution.\n\\[f(d_{Ai} | n_{Ai}, n_{Bi}, d_i) = \\frac{{{n_{Ai}}\\choose{d_{Ai}}}{{n_{Bi}}\\choose{d_{Bi}}}}{{n_i}\\choose{d_i}}\\]\nThe expected value is \\(e_{Ai} = E(d_{Ai}) = \\frac{d_i}{n_i} \\cdot n_{0i}\\) with variance \\(v_{Ai} = Var(d_{Ai}) = d_{i} \\cdot \\frac{n_{Ai}}{n_i}  \\cdot \\frac{n_{1i}}{n_i} \\cdot \\frac{n_i - d_i}{n_i - 1}\\).\nThe log-rank test statistic is the sum of the differences between the observed and expected events, \\(U_0 = \\sum (d_{Ai} - e_{Ai})\\), normalized by dividing by the square-root of its variance, \\(V_0 = Var({U_0}) = \\sum v_{Ai}\\).\n\\[U = \\frac{U_0}{\\sqrt{V_0}} \\sim N(0, 1)\\]\n\\(U^2\\) is a chi-square random variable with one degree of freedom.\n\\[U^2 = \\frac{U_0^2}{V_0} \\sim \\chi_1^2\\]\n\nlung_1 &lt;- survival::lung %&gt;%\n  mutate(sex = factor(sex, levels = c(1, 2), labels = c(\"Male\", \"Female\")))\n(km_diff &lt;- survdiff(Surv(time, status) ~ sex, data = lung_1))\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ sex, data = lung_1)\n\n             N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=Male   138      112     91.6      4.55      10.3\nsex=Female  90       53     73.4      5.68      10.3\n\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 \n\n\nThe p-value for \\(\\chi_1^2\\) = 10.3 is 1 - pchisq(km_diff$chisq, length(km_diff$n) - 1) = 0.001, so reject \\(H_0\\) that males and females have identical survival patterns.\nWhile the log-rank test can determine whether survival differs between groups, it does not estimate the effect size. It is a statistical, but not a clinical, assessment of the factor’s impact (Bradburn 2003).\n\n\n\n\n\n\nBradburn, Clark, M. J. 2003. “Survival Analysis Part II: Multivariate Data Analysis–an Introduction to Concepts and Methods.” British Journal of Cancer 89 (3). https://doi.org/10.1038/sj.bjc.6601119.\n\n\nClark, Bradburn, T. G. 2003. “Survival Analysis Part i: Basic Concepts and First Analyses.” British Journal of Cancer 89 (2). https://doi.org/10.1038/sj.bjc.6601118.\n\n\nSuresh, Severn, K. 2022. “Survival Prediction Models: An Introduction to Discrete-Time Modeling.” BMC Med Res Methodol 22 (207). https://doi.org/10.1186/s12874-022-01679-6.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts</span>"
    ]
  },
  {
    "objectID": "01-concepts.html#footnotes",
    "href": "01-concepts.html#footnotes",
    "title": "1  Concepts",
    "section": "",
    "text": "Clark (2003) has discussion of censoring types. Right censoring is most common.↩︎\nMany texts represent hazard with the \\(\\lambda\\) symbol instead of \\(h\\).↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Concepts</span>"
    ]
  },
  {
    "objectID": "02-km.html",
    "href": "02-km.html",
    "title": "2  Kaplan-Meier",
    "section": "",
    "text": "2.1 Assumptions\nKaplan-Meier (KM) is an estimator of the survivor function, \\(S(t)\\), the probability of surviving beyond time t. KM is defined as the sum-product of the conditional probabilities of surviving to the next failure time, i.e., the cumulative survival probability.\n\\[\\hat{S}(t) = \\prod_{i: t_i &lt; t}{\\frac{n_i - d_i}{n_i}} \\tag{2.1}\\]\nIn Equation 2.1, \\(n_i\\) is the number of subjects at risk at time \\(t_i\\) and \\(d_i\\) is the number incurring the event. The KM curve falls only when an event occurs, not when a subject is censored. Confidence limits are calculated using the “delta” method to obtain the variance of \\(\\log \\hat{S}(t)\\).1\nAfter a KM analysis, a log-rank test is often used to test for differences among curves. KM is ideal for comparing survival between groups defined with a single categorical variable because it makes no assumptions about the underlying hazard function. Multivariate analyses require parametric and semi-parametric models that estimate the hazard function, \\(h(t)\\), the instantaneous event rate at t given survival up to t (Clark 2003). Even there, KM is a useful exploratory step because it shows the actual survival probabilities.\nThe KM method is demonstrated here with a case study using the survival::lung dataset. The study investigated differences in all-cause mortality between men and women diagnosed with advanced lung cancer. 227 participants aged 39 to 82 were monitored up to three years until time of death. The participants were segmented into three groups according to their ECOG performance score: Asymptomatic, symptomatic but completely Ambulatory, and Bedridden. The dataset includes participant age and gender (not used in this analysis). Table 2.1 presents the summary statistics.\nKM analyses model processes that have a binary outcome, a precise event time, minimal left censoring (unknown starting times), and censoring that is independent of the event.2\nTwo assumptions underlie the KM procedure:\nThe participant censoring plot shows censored cases were equally spread over time and not too dissimilar for the Asymptomatic and Ambulatory groups, but the Bedridden group had only a few censoring events (Figure 2.1).\nShow the code\nd_lung |&gt;\n  filter(status == \"censored\") |&gt;\n  ggplot(aes(x = time, y = fct_rev(ph.ecog))) +\n  geom_point() +\n  theme_light() +\n  labs(title = \"Participant Censoring\", x = \"Time (days)\", y = NULL)\n\n\n\n\n\n\n\n\nFigure 2.1: Censored cases were equally spread over time\nFrom this, conclude that censored cases were negatively associated with symptom severity, Asymptomatic, 26 (41%), Ambulatory, 31 (27%), and Bedridden, 6 (12%) study groups.3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kaplan-Meier</span>"
    ]
  },
  {
    "objectID": "02-km.html#assumptions",
    "href": "02-km.html#assumptions",
    "title": "2  Kaplan-Meier",
    "section": "",
    "text": "No Cohort Effects. If starting times are staggered and encompass the introduction of new therapies, survival will vary. Test for cohort effects by running KM tests for multiple time intervals. (Only test for cohort affects if there is reason to believe they exist.)\nSimilar Censorship Patterns. The amount and pattern of censoring should be similar. Censorship patterns can be tested by inspection.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kaplan-Meier</span>"
    ]
  },
  {
    "objectID": "02-km.html#fitting-the-model",
    "href": "02-km.html#fitting-the-model",
    "title": "2  Kaplan-Meier",
    "section": "2.2 Fitting the Model",
    "text": "2.2 Fitting the Model\nCalculate \\(\\hat{S}(t)\\) with survival::survfit(). survfit() operates on a Surv object, created by survival::Surv(). Explanatory variables can be defined as factors, but the event indicator, status, must be numeric or boolean, coded as 0|1, 1|2, or FALSE|TRUE.4 If the confidence interval crosses zero, specify the log-log transformation parameter conf.type = \"log-log\" (this one doesn’t need it).\n\n(km_fit &lt;- survfit(Surv(time, status == \"died\") ~ ph.ecog, data = d_lung))\n\nCall: survfit(formula = Surv(time, status == \"died\") ~ ph.ecog, data = d_lung)\n\n                       n events median 0.95LCL 0.95UCL\nph.ecog=Asymptomatic  63     37    394     348     574\nph.ecog=Ambulatory   113     82    306     268     429\nph.ecog=Bedridden     51     45    183     153     288\n\n\n37 of the 63 Asymptomatic subjects died, 82 of the 113 Ambulatory subjects died, and 45 of the 51 Bedridden subjects died. gtsummary::tbl_survfit() presents a similar summary in a formatted table. Table 2.2 additionally presents the effects of sex and age.\n\n(km_gt &lt;- gtsummary::tbl_survfit(\n  list(\n    survfit(Surv(time, status == \"died\") ~ 1, data = d_lung),\n    survfit(Surv(time, status == \"died\") ~ ph.ecog, data = d_lung),\n    survfit(Surv(time, status == \"died\") ~ sex, data = d_lung),\n    survfit(Surv(time, status == \"died\") ~ age_bin, data = d_lung)\n  ),\n  probs = 0.5,\n  label_header = \"**Median Survival**\"\n))\n\n\n\nTable 2.2: Kaplan-Meier model fit.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nMedian Survival\n\n\n\n\nOverall\n310 (285, 363)\n\n\nph.ecog\n\n\n\n\n    Asymptomatic\n394 (348, 574)\n\n\n    Ambulatory\n306 (268, 429)\n\n\n    Bedridden\n183 (153, 288)\n\n\nsex\n\n\n\n\n    Male\n270 (218, 320)\n\n\n    Female\n426 (348, 550)\n\n\nage_bin\n\n\n\n\n    38,55]\n320 (226, 533)\n\n\n    55,65]\n348 (285, 477)\n\n\n    65,83]\n301 (267, 361)\n\n\n\n\n\n\n\n\n\n\n\nValues are median with 95% CI. Lower levels of ECOG appear to have a protective effect. Asymptomatic participants had a median survival time of 394 (348, 574) days. This was longer than the Ambulatory group, 306 (268, 429) days, and Bedridden group, 183 (153, 288) days.\nExtract values for reporting with gtsummary::inline_text(), or create a summary object. The table attribute of the summary object is a named numeric matrix. E.g., get Asymptomatic events with km_smry$table[\"ph.ecog=Asymptomatic\", \"events\"].\n\nkm_smry &lt;- summary(km_fit)\n\n# Example: Asymptomatic median survival time.\nkm_smry$table[\"ph.ecog=Asymptomatic\", \"median\"]\n## [1] 394\n\nYou can also use summary() with the time parameter to estimate survival up until a point in time.\n\nsummary(km_fit, time = 500)\n\nCall: survfit(formula = Surv(time, status == \"died\") ~ ph.ecog, data = d_lung)\n\n                ph.ecog=Asymptomatic \n        time       n.risk      n.event     survival      std.err lower 95% CI \n    500.0000      13.0000      30.0000       0.3745       0.0751       0.2528 \nupper 95% CI \n      0.5548 \n\n                ph.ecog=Ambulatory \n        time       n.risk      n.event     survival      std.err lower 95% CI \n    500.0000      21.0000      67.0000       0.3054       0.0505       0.2209 \nupper 95% CI \n      0.4223 \n\n                ph.ecog=Bedridden \n        time       n.risk      n.event     survival      std.err lower 95% CI \n    500.0000       7.0000      40.0000       0.1635       0.0552       0.0844 \nupper 95% CI \n      0.3169 \n\n\ngtsummary::tbl_survfit() does something similar.\n\ngtsummary::tbl_survfit(km_fit, times = 500)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nTime 500\n\n\n\n\nph.ecog\n\n\n\n\n    Asymptomatic\n37% (25%, 55%)\n\n\n    Ambulatory\n31% (22%, 42%)\n\n\n    Bedridden\n16% (8.4%, 32%)\n\n\n\n\n\n\n\nbroom::tidy() summarizes the data by each event time. At time t = 5, 1 of the 63 Asymptomatic ECOG at risk died, so \\(S(5) = 1 - \\frac{1}{63} = .9841\\). At t = 11, 1 of the 62 that remained died, so \\(S(11) = S(5) \\cdot \\frac{1}{62} = .9841 \\cdot .9839 = .9683\\), and so on. This is the support for the survival curves.\n\nbroom::tidy(km_fit)\n\n# A tibble: 216 × 9\n    time n.risk n.event n.censor estimate std.error conf.high conf.low strata   \n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    \n 1     5     63       1        0    0.984    0.0160     1        0.954 ph.ecog=…\n 2    11     62       1        0    0.968    0.0228     1        0.926 ph.ecog=…\n 3    15     61       1        0    0.952    0.0282     1        0.901 ph.ecog=…\n 4    31     60       1        0    0.937    0.0328     0.999    0.878 ph.ecog=…\n 5    53     59       1        0    0.921    0.0370     0.990    0.856 ph.ecog=…\n 6    65     58       1        0    0.905    0.0409     0.980    0.835 ph.ecog=…\n 7    81     57       1        0    0.889    0.0445     0.970    0.815 ph.ecog=…\n 8   147     56       1        0    0.873    0.0480     0.959    0.795 ph.ecog=…\n 9   166     55       1        0    0.857    0.0514     0.948    0.775 ph.ecog=…\n10   175     54       0        1    0.857    0.0514     0.948    0.775 ph.ecog=…\n# ℹ 206 more rows\n\n\nThe KM plot in Figure 2.2 gives you a better feel for the data. Vertical drops indicate events and vertical ticks indicate censoring. Cumulative survival is negatively associated with the ECOG performance score. There is no substantial crossing of the survival curves that would affect the power of the statistical tests. The curves are similarly shaped.\n\n\n\n\n\n\nNote\n\n\n\nsurvminer does a good job plotting KM models and includes a risk table, but ggplot allows you to control more of the output. Figure 2.3 combines ggplot with giraph for a full risk table in the tooltips.\n\n\n\nsurvminerggplot\n\n\n\n\nShow the code\nggsurvplot(\n  km_fit,\n  data = d_lung,\n  fun = \"pct\",\n  risk.table = TRUE,\n  fontsize = 3, # used in risk table\n  surv.median.line = \"hv\", # median horizontal and vertical ref lines\n  palette = my_palette$warm,\n  title = \"Kaplan-Meier Survival Function Estimate\",\n  legend.title = \"ECOG\",\n  legend.labs = levels(d_lung$ph.ecog)\n)\n\n\n\n\n\n\n\n\nFigure 2.2: Kaplan-Meier survival curve.\n\n\n\n\n\n\n\n\n\nShow the code\nmed &lt;-\n  broom::tidy(km_fit) |&gt;\n  filter(estimate &lt; .5) |&gt;\n  slice_max(estimate, by = strata) |&gt;\n  mutate(strata = str_remove(strata, \"ph.ecog=\"))\np &lt;-\n  broom::tidy(km_fit) |&gt;\n  mutate(\n    strata = str_remove(strata, \"ph.ecog=\"),\n    tt = glue(\"{strata}\\n\\nTime: {time}\\nAt risk: {n.risk}\\nEst.: \",\n              \"{percent(estimate, .1)}\")\n  ) |&gt;\n  ggplot(aes(x = time, y = estimate, color = strata)) +\n  geom_step() +\n  geom_segment(\n    data = med,\n    aes(x = 0, xend = time, y = .5, yend = .5),\n    linetype = 2\n  ) +\n  geom_segment(\n    data = med,\n    aes(x = time, xend = time, y = 0, yend = .5),\n    linetype = 2\n  ) +\n  geom_point_interactive(aes(tooltip = tt)) +\n  scale_color_manual(values = my_palette$warm) +\n  scale_y_continuous(labels = percent_format(1)) +\n  labs(\n    x = \"time (days)\", y = \"Survival probability\", color = NULL,\n    title = \"Kaplan-Meier Survival Function Estimate\"\n  )\ngirafe(ggobj = p)\n\n\n\n\n\n\n\n\nFigure 2.3: Kaplan-Meier survival curve using ggplot.\n\n\n\n\n\n\n\nggsurvplot() can also plot the cumulative risk function, \\(F(t) = 1 - S(t)\\), with parameter fun = \"event\" (Figure 2.4), and the cumulative hazard function, \\(H(t) = -\\log S(t)\\), with parameter fun = \"cumhaz\" (Figure 2.5).\n\n\n\n\n\nShow the code\nggsurvplot(\n  km_fit,\n  data = d_lung,\n  fun = \"event\",\n  linetype = \"strata\",\n  pval = FALSE,\n  conf.int = FALSE,\n  palette = my_palette$warm\n)\n\n\n\n\n\n\n\n\nFigure 2.4: Cumulative risk curve.\n\n\n\n\n\n\n\n\n\nShow the code\nggsurvplot(\n  km_fit,\n  data = d_lung,\n  fun = \"cumhaz\",\n  linetype = \"strata\", # Change line type by groups\n  pval = FALSE,\n  conf.int = FALSE,\n  palette = my_palette$warm\n)\n\n\n\n\n\n\n\n\nFigure 2.5: Cumulative hazard curve.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kaplan-Meier</span>"
    ]
  },
  {
    "objectID": "02-km.html#interpreting-results",
    "href": "02-km.html#interpreting-results",
    "title": "2  Kaplan-Meier",
    "section": "2.3 Interpreting Results",
    "text": "2.3 Interpreting Results\nDetermine whether there are significant differences in the fitted survival distributions using a log-rank test (and/or Breslow and Tarone-Ware test). If there are differences, run a pairwise comparison post-hoc test to determine which curves differ.\nThe log-rank test weights the difference at each time point equally. Compared to Breslow and Tarone-Ware, it places greater emphasis on differences at later rather than earlier time points. The Breslow test (aka generalized Wilcoxon or Gehan) weights the differences by the number at risk at each time point. The effect is to place greater weight on the differences at earlier time points. The Tarone-Ware test weights differences the same way as Breslow, but takes the square root of the number at risk.\n\nLog-rankBreslowTarone-Ware\n\n\n\n(km_diff &lt;- survdiff(\n  Surv(time, status == \"died\") ~ ph.ecog, \n  data = d_lung\n))\n\nCall:\nsurvdiff(formula = Surv(time, status == \"died\") ~ ph.ecog, data = d_lung)\n\n                       N Observed Expected (O-E)^2/E (O-E)^2/V\nph.ecog=Asymptomatic  63       37     54.2    5.4331    8.2119\nph.ecog=Ambulatory   113       82     83.5    0.0279    0.0573\nph.ecog=Bedridden     51       45     26.3   13.2582   15.9641\n\n Chisq= 19  on 2 degrees of freedom, p= 8e-05 \n\n\nThe survival distributions for the three interventions were statistically significantly different, \\(\\chi^2\\)(2) = 19.0, p &lt; .001.\n\n\n\ncoin::logrank_test(\n  Surv(time, status == \"died\") ~ ph.ecog,\n  data = d_lung,\n  type = \"Gehan-Breslow\"\n)\n\n\n    Asymptotic K-Sample Gehan-Breslow Test\n\ndata:  Surv(time, status == \"died\") by\n     ph.ecog (Asymptomatic, Ambulatory, Bedridden)\nchi-squared = 19.431, df = 2, p-value = 6.034e-05\n\n\n\n\n\ncoin::logrank_test(\n  Surv(time, status == \"died\") ~ ph.ecog,\n  data = d_lung,\n  type = \"Tarone-Ware\"\n)\n\n\n    Asymptotic K-Sample Tarone-Ware Test\n\ndata:  Surv(time, status == \"died\") by\n     ph.ecog (Asymptomatic, Ambulatory, Bedridden)\nchi-squared = 19.009, df = 2, p-value = 7.451e-05\n\n\n\n\n\nThe three tests produced identical conclusions. The log rank test is an omnibus test. Create a pairwise comparisons table to see which groups differed.\n\n(km_pairwise &lt;- survminer::pairwise_survdiff(\n  Surv(time, status == \"died\") ~ ph.ecog,\n  data = d_lung\n))\n\n\n    Pairwise comparisons using Log-Rank test \n\ndata:  d_lung and ph.ecog \n\n           Asymptomatic Ambulatory\nAmbulatory 0.0630       -         \nBedridden  9e-05        0.0039    \n\nP value adjustment method: BH \n\n\nAdjust the statistical significance to compensate for making multiple comparisons with a Bonferroni correction. There are three comparisons so divide .05 by 3, so the significance threshold is p &lt; .0167. There was a statistically significant difference in survival distributions for Asymptomatic vs Bedridden, p &lt; .001, and Ambulatory vs Bedridden, p = 0.004, but not for Asymptomatic vs Ambulatory, p = 0.063.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kaplan-Meier</span>"
    ]
  },
  {
    "objectID": "02-km.html#reporting",
    "href": "02-km.html#reporting",
    "title": "2  Kaplan-Meier",
    "section": "2.4 Reporting",
    "text": "2.4 Reporting\nThe guidelines for reporting the Kaplan-Meier test are from Laerd’s Kaplan-Meier using SPSS Statistics (Laerd 2015). Report the results like this.\n\n227 Men and women diagnosed with advanced lung cancer aged 39 to 82 were monitored up to three years until time of death. Participants were classified into three groups according to their ECOG performance score: asymptomatic (n = 63), symptomatic but completely ambulatory (n = 113), and bedridden at least part of the day (n = 51). A Kaplan-Meier survival analysis (Kaplan & Meier, 1958) was conducted to compare survival times among the three ECOG performance scores. Censored cases were negatively associated with symptom severity, asymptomatic, 26 (41%), symptomatic but completely ambulatory, 31 (27%), and bedridden, 6 (12%) study groups. Participants that were asymptomatic had a median survival time of 394 (348, 574) days. This was longer than the ambulatory group, 306 (268, 429) days, and bedridden group, 183 (153, 288) days. A log rank test was run to determine if there were differences in the survival distribution for the different types of intervention. The survival distributions for the three interventions were statistically significantly different, \\(\\chi^2\\)(2) = 19.0, p &lt; .001. Pairwise log rank comparisons were conducted to determine which intervention groups had different survival distributions. A Bonferroni correction was made with statistical significance accepted at the p &lt; .017 level. There was a statistically significant difference in survival distributions for the asymptomatic vs bedridden, p &lt; .001, and ambulatory vs bedridden, p = 0.004, groups. However, the survival distributions for the asymptomatic vs ambulatory group were not statistically significant, p = 0.063\n\n\n\n\n\n\n\nClark, Bradburn, T. G. 2003. “Survival Analysis Part i: Basic Concepts and First Analyses.” British Journal of Cancer 89 (2). https://doi.org/10.1038/sj.bjc.6601118.\n\n\nLaerd. 2015. Statistical Tutorials and Software Guides. https://statistics.laerd.com/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kaplan-Meier</span>"
    ]
  },
  {
    "objectID": "02-km.html#footnotes",
    "href": "02-km.html#footnotes",
    "title": "2  Kaplan-Meier",
    "section": "",
    "text": "The delta method approximates the variance of a function of an estimator using a Taylor expansion.↩︎\nWith uninformative censoring, censoring causes are independent of the event. Subjects do not drop out of the study because of something related to their group. E.g., a subject does not drop out of a therapy study because the therapy is making their condition worse.↩︎\nUse gtsummary::inline_text() to summarize (e.g., gtsummary::inline_text(t1, variable = status, level = \"censored\", column = \"Asymptomatic\").↩︎\nSee neat discussion in Note section of Surv() help file.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Kaplan-Meier</span>"
    ]
  },
  {
    "objectID": "03-parametric.html",
    "href": "03-parametric.html",
    "title": "3  Parametric",
    "section": "",
    "text": "3.1 Exponential\nThe KM curve with logrank test is a univariate analyses describing the effect of a single categorical factor variable on survival. Parametric models, on the other hand, are able to describe the effect of multiple covariates. Fully-parametric models are less common than the semi-parametric Cox model (Chapter 4) because they are less flexible, but if the process follows the parametric distribution these models are preferable because they estimate fewer parameters, allow you to extrapolate beyond the range of the data, and produce a more meaningful interpretation of the underlying mechanism in the model (The Analysis Factor, Zhang (2016)).\nDevin Incerti and Tavish Srivastava review parametric distributions commonly used in survival analyses. Table 3.1 below is a selected list of distributions that are reviewed in both articles and are included in the survival package. Not included below, but available in survival or flexsurv are the Gompertz, Gamma, log-logistic, and generalized gamma.\nChoose the distribution that best matches your a priori beliefs about the hazard function or choose the one with the lowest AIC (The Analysis Factor).\nThe exponential distribution (probability notes), \\(T \\sim \\mathrm{Exp}(\\lambda)\\), is the easiest to use because its hazard function is time-independent. @ref(fig:fig03-exp) shows a constant hazard function at two rates, \\(\\lambda \\in [1, 3]\\), per unit of time.\nexpand.grid(\n  t = seq(0.1, 10, .1),\n  loc = c(1, 3)\n) %&gt;% \n  mutate(\n    `F (CDF)` = pexp(t, rate = loc),\n    `f (PDF)` = dexp(t, rate = loc),\n    S = 1 - `F (CDF)`,\n    `h = f / S` = flexsurv::hexp(t, rate = loc),\n    loc = paste(\"rate:\", loc)\n) %&gt;%\n  plot_dist()\n\n\n\n\nThe exponential distribution at two rates (1, 2).\n\\[\\begin{eqnarray}\n\\log h(t) &=& \\alpha + \\beta X \\\\\nh(t) &=& e^{\\left(\\alpha + \\beta X \\right)} \\\\\n&=& \\lambda\n\\end{eqnarray}\\]\nInterpret \\(\\alpha\\) as the baseline log-hazard because when \\(X\\) is zero \\(h(t) = e^\\alpha\\). The cumulative hazard is \\(H(t) = \\int_0^t \\lambda dt = \\lambda t\\) and the corresponding survival function is\n\\[S(t) = e^{-H(t)} = e^{-\\lambda t}.\\]\nThe expected survival time is \\(E(T) = \\int_0^\\infty S(t)dt = \\int_0^\\infty e^{-\\lambda t} dt = 1 / \\lambda.\\) The median survival time is \\(S(t) = e^{-\\lambda t} = 0.5\\), or \\(t_{med} = \\log(2) / \\lambda\\).\nThe survival curve is fit using maximimum likelihood estimation (MLE). My statistics notes explain MLE for the exponential distribution. Survival curve MLE is a little more complicated because of censoring. The likelihood \\(L\\) that \\(\\lambda\\) produces the observed outcomes is the product of the probability densities for each observation because they are a sequence of independent variables. Let \\(\\delta_i = [1, 0]\\) for unsensored and censored observations.\n\\[\nL(\\lambda; t_1, t_2, \\dots, t_n) = \\Pi_{i=1}^n f(t_i; \\lambda)^{\\delta_i} S(t_i; \\lambda)^{1-\\delta_i}\n\\]\nSubstituting \\(f(t) = h(t) S(t)\\), and then substituting \\(h(t) = \\lambda\\) and \\(S(t) = e^{-\\lambda t}\\) and simplifying,\n\\[\\begin{eqnarray}\nL(\\lambda; t_1, t_2, \\dots, t_n) &=& \\Pi_{i=1}^n h(t_i; \\lambda)^{\\delta_i} S(t_i; \\lambda) \\\\\n&=& \\Pi_{i=1}^n \\lambda^{\\delta_1} e^{-\\lambda t_i} \\\\\n&=& \\lambda^{\\sum \\delta_i} \\exp \\left(-\\lambda \\sum_{i=1}^n t_i \\right)\n\\end{eqnarray}\\]\nSimplify the notation by letting \\(d = \\sum \\delta_i\\), the total number of events (or deaths or whatever), and \\(V = \\sum t_i\\), the number of person-years (or days or whatever).\n\\[L(\\lambda; t_1, t_2, \\dots, t_n) = \\lambda^d e^{-\\lambda V}\\]\nThis form is difficult to optimize, but the log of it is simple.\n\\[l(\\lambda; t_1, t_2, \\dots, t_n) = d \\log(\\lambda) - \\lambda V\\]\nMaximize the log-likelihood equation by setting its derivative to zero and solving for \\(\\lambda\\).\n\\[\\begin{eqnarray}\n\\frac{d}{d \\lambda} l(\\lambda; t_1, t_2, \\dots, t_n) &=& \\frac{d}{d \\lambda} \\left(d \\log(\\lambda) - \\lambda V \\right) \\\\\n0 &=& \\frac{d}{\\lambda} - V \\\\\n\\lambda &=& \\frac{d}{V}\n\\end{eqnarray}\\]\n\\(\\lambda\\) is the reciprocal of the sample mean, person-years divided by failures.\nThe second derivative, \\(-\\frac{d}{\\lambda^2}\\), is approximately the negative of the variance of \\(\\lambda\\).\n\\[V(\\lambda) = d / V^2\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parametric</span>"
    ]
  },
  {
    "objectID": "03-parametric.html#weibull",
    "href": "03-parametric.html#weibull",
    "title": "3  Parametric",
    "section": "3.2 Weibull",
    "text": "3.2 Weibull\nAlthough the exponential function is convenient, the Weibull distribution is more appropriate for modeling lifetimes.\n\nexpand.grid(\n  t = seq(0.1, 10, .1),\n  loc = c(.5, 1, 1.5, 2)\n) %&gt;% \n  mutate(\n    `F (CDF)` = pweibull(t, shape = loc, scale = 2),\n    `f (PDF)` = dweibull(t, shape = loc, scale = 2),\n    S = 1 - `F (CDF)`,\n    `h = f / S` = flexsurv::hweibull(t, shape = loc, scale = 2),\n    loc = paste(\"shape:\", loc)\n) %&gt;%\n  plot_dist()\n\n\n\n\nThe Weibull distribution at four rates (shapes) (0.5, 1, 1.5, 2).\n\n\n\n\nIts hazard function is\n\\[\\begin{eqnarray}\nh(t) &=& \\alpha \\lambda (\\lambda t)^{\\alpha - 1} \\\\\n&=& \\alpha \\lambda^\\alpha t^{\\alpha-1}\n\\end{eqnarray}\\]\nThe cumulative hazard function is \\(H(t) = (\\lambda t)^\\alpha\\) and the corresponding survival function is\n\\[S(t) = e^{-(\\lambda t)^\\alpha}.\\]\nThe exponential distribution is a special case of the Weibull where \\(\\alpha = 1\\). The expected survival time is \\(E(t) = \\frac{\\Gamma (1 + 1 / \\alpha)}{\\lambda}\\). The median survival time is \\(t_{med} = \\frac{[\\log(2)]^{1 / \\alpha}}{\\lambda}\\).\nTo measure the effects of covariates, it is preferable to substitute \\(\\sigma = 1 / \\alpha\\) and \\(\\mu = -\\log \\lambda\\) so\n\\[\nh(t) = \\frac{1}{\\sigma} e^{-\\frac{\\mu}{\\sigma}} t^{\\frac{1}{\\sigma} - 1}\n\\]\nand\n\\[\nS(t) = e^{-e^{-\\mu/\\sigma}t^{1/\\sigma}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parametric</span>"
    ]
  },
  {
    "objectID": "03-parametric.html#log-normal",
    "href": "03-parametric.html#log-normal",
    "title": "3  Parametric",
    "section": "3.3 Log-normal",
    "text": "3.3 Log-normal\n\nexpand.grid(\n  t = seq(0.1, 10, .1),\n  loc = c(.5, 1, 1.5, 2)\n) %&gt;% \n  mutate(\n    `F (CDF)` = plnorm(t, meanlog = loc),\n    `f (PDF)` = dlnorm(t, meanlog = loc),\n    S = 1 - `F (CDF)`,\n    `h = f / S` = flexsurv::hlnorm(t, meanlog = loc),\n    loc = paste(\"Mean Log:\", loc)\n) %&gt;%\n  plot_dist()\n\n\n\n\nThe Log-normal distribution at four rates (mean logs) (0.5, 1, 1.5, 2).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parametric</span>"
    ]
  },
  {
    "objectID": "03-parametric.html#distribution-selection",
    "href": "03-parametric.html#distribution-selection",
    "title": "3  Parametric",
    "section": "3.4 Distribution Selection",
    "text": "3.4 Distribution Selection\nUse the flexsurv package to model parametric distributions. This section demonstrates how to choose the distribution with the lung cancer data set from the survival package. lung records the status (1 censored, 2 dead) of 228 patients with advanced lung cancer. There are several covariates in the data set, but for this illustration we’ll include just ph.ecog, the patient’s level of functioning (0 = good, 5 = dead) and treat it as a factor variable.\n\ndf_lung &lt;- survival::lung %&gt;% \n  mutate(ph.ecog = factor(ph.ecog)) %&gt;%\n  select(time, status, ph.ecog)\nhead(df_lung)\n\n  time status ph.ecog\n1  306      2       1\n2  455      2       0\n3 1010      1       0\n4  210      2       1\n5  883      2       0\n6 1022      1       1\n\n\nFigure @ref(fig:fig03-1) is the Kaplan-Meier cumulative hazard function estimate. The cumulative hazard rises more sharply at around time 500 then changes inflection again around time 700.\n\nkm_fit &lt;- survfit(Surv(time, status) ~ 1, data = df_lung)\n\nggsurvplot(\n  km_fit,\n  data = df_lung,\n  fun = \"cumhaz\",\n  pval.method = TRUE,\n  conf.int = TRUE,\n  ggtheme = theme_light(),\n  title = \"Kaplan-Meier Cumulative Hazard Function Estimate\"\n)\n\n\n\n\nKaplan-Meier Cumulative Hazard Function Estimate.\n\n\n\n\nUnfortunately, survminer::ggsurplot() does not plot the instantaneous hazard function. But the epR::epi.insthaz() does calculate instantaneous hazards. Figure @ref(fig:fig03-2) is the hazard function we want to approximate with a parametric distribution.\n\nepiR::epi.insthaz(km_fit) %&gt;%\n  ggplot(aes(x = time, y = hest)) + \n  geom_smooth(color = \"red\", method = \"loess\", formula = \"y ~ x\") +\n  theme_light() +\n  labs(title = \"Kaplan-Meier Hazard Function Estimate\", \n       x = \"Time\", y = \"Instantaneous Hazard\")\n\n\n\n\nKaplan-Meier Hazard Function Estimate\n\n\n\n\nUse the flexsurv packages to estimate parametric survival curves.\n\npar_fits &lt;- tibble(\n  dist_param = c(\"exp\", \"weibull\", \"gompertz\", \"gamma\", \"lognormal\", \"llogis\", \n                 \"gengamma\"),\n  dist_name = c(\"Exponential\", \"Weibull (AFT)\", \"Gompertz\", \"Gamma\", \n                \"Lognormal\", \"Log-logistic\", \"Generalized gamma\")\n) %&gt;%\n  mutate(\n    fit = map(dist_param, ~flexsurvreg(Surv(time, status) ~ 1, data = df_lung, dist = .x)),\n    fit_smry = map(fit, ~summary(.x, type = \"hazard\", ci = FALSE, tidy = TRUE)),\n    AIC = map_dbl(fit, ~.x$AIC)\n  )\n\nFigure @ref(fig:fig03-3) shows the fitted curves. Which most closely resembles the KM hazard plot?\n\npar_fits %&gt;%\n  select(-c(dist_param, fit)) %&gt;%\n  unnest(fit_smry) %&gt;%\n  ggplot(aes(x = time, y = est, color = dist_name)) +\n  geom_line() +\n  theme_light() +\n  labs(title = \"Parametric Distribution Fits to Lung Cancer Data.\")\n\n\n\n\nParametric fits.\n\n\n\n\nNone of them seem great, but the bottom three curves (Exponential, Log-logistic, and Log-normal) seem especially poor choices. Using the AIC, best fit appears to be the Weibull model.\n\npar_fits %&gt;%\n  arrange(AIC) %&gt;%\n  select(dist_name, AIC)\n\n# A tibble: 7 × 2\n  dist_name           AIC\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Weibull (AFT)     2312.\n2 Generalized gamma 2313.\n3 Gamma             2313.\n4 Gompertz          2315.\n5 Log-logistic      2326.\n6 Exponential       2327.\n7 Lognormal         2343.\n\n\nUse the Weibull (AFT) model then. What changes if you add ph.ecog to the model?\n\nweibull_fit &lt;- flexsurvreg(Surv(time, status) ~ ph.ecog, data = df_lung, dist = \"weibull\")\nweibull_fit\n\nCall:\nflexsurvreg(formula = Surv(time, status) ~ ph.ecog, data = df_lung, \n    dist = \"weibull\")\n\nEstimates: \n          data mean  est        L95%       U95%       se         exp(est) \nshape            NA    1.36104    1.20529    1.53692    0.08439         NA\nscale            NA  555.41821  437.97614  704.35204   67.31932         NA\nph.ecog1    0.49780   -0.26405   -0.55005    0.02196    0.14592    0.76794\nph.ecog2    0.22026   -0.66915   -0.99442   -0.34388    0.16596    0.51214\nph.ecog3    0.00441   -1.54904   -3.00855   -0.08952    0.74466    0.21245\n          L95%       U95%     \nshape            NA         NA\nscale            NA         NA\nph.ecog1    0.57692    1.02220\nph.ecog2    0.36994    0.70902\nph.ecog3    0.04936    0.91437\n\nN = 227,  Events: 164,  Censored: 63\nTotal time at risk: 69522\nLog-likelihood = -1138.334, df = 5\nAIC = 2286.667\n\n\n`flexsurvreg() defaults to only modeling the relationship between the covariates and the location parameter of the distribution. You can test whether the covariates affect the shape too by specifying the relationship with the anc parameter.\n\nweibull_fit_shape &lt;- flexsurvreg(\n  Surv(time, status) ~ ph.ecog, \n  # anc = list(shape = ~ph.ecog),\n  data = df_lung, dist = \"weibull\")\nweibull_fit_shape\n\nCall:\nflexsurvreg(formula = Surv(time, status) ~ ph.ecog, data = df_lung, \n    dist = \"weibull\")\n\nEstimates: \n          data mean  est        L95%       U95%       se         exp(est) \nshape            NA    1.36104    1.20529    1.53692    0.08439         NA\nscale            NA  555.41821  437.97614  704.35204   67.31932         NA\nph.ecog1    0.49780   -0.26405   -0.55005    0.02196    0.14592    0.76794\nph.ecog2    0.22026   -0.66915   -0.99442   -0.34388    0.16596    0.51214\nph.ecog3    0.00441   -1.54904   -3.00855   -0.08952    0.74466    0.21245\n          L95%       U95%     \nshape            NA         NA\nscale            NA         NA\nph.ecog1    0.57692    1.02220\nph.ecog2    0.36994    0.70902\nph.ecog3    0.04936    0.91437\n\nN = 227,  Events: 164,  Censored: 63\nTotal time at risk: 69522\nLog-likelihood = -1138.334, df = 5\nAIC = 2286.667\n\n\nThe 95% CIs for the shape estimators contain 0, so they do not belong in the model. Sticking with the original model, plot the effect of ECOG on survival.\n\nsummary(weibull_fit, \n        newdata = list(ph.ecog = levels(df_lung$ph.ecog)), \n        type = \"hazard\", tidy = TRUE) %&gt;%\n  ggplot(aes(x = time)) +\n  geom_line(aes(y = est, col = ph.ecog)) +\n  theme_light() +\n  theme(legend.position = \"bottom\") +\n  labs(x = \"Days\", y = \"Hazard\", color = \"ECOG Performance Score\",\n       title = \"Fitted Values Plot\") \n\n\n\n\n\n\n\n\nThe log of the negative log of \\(S\\), \\(\\log[-\\log(S_i)] = \\alpha \\log(\\lambda) + \\alpha \\log(t_i) = \\frac{\\mu}{\\sigma} + \\frac{1}{\\sigma} \\log(t_i)\\) is a linear function, so you can use it to determine whether the Weibull function is appropriate for your analysis. Return to the lung data set introduced in Kaplan-Meier section. Use the Kaplan-Meier estimate of the survival distribution to extract the survival estimates and each time, transform them to conform to the above equation, and fit a linear model.\n\nkm_fit_1 &lt;- survfit(Surv(time, status) ~ 1, data = lung)\nlog_log_s &lt;- log(-log(km_fit_1$surv))\nlog_t &lt;- log(km_fit_1$time)\n\nkm_fit_1_lm &lt;- lm(log_log_s ~ log_t)\nkm_fit_1_lm %&gt;%\n  broom::augment() %&gt;%\n  ggplot(aes(x = log_t)) +\n  geom_point(aes(y = log_log_s)) +\n  geom_line(aes(y = .fitted), linetype = 2, color = \"goldenrod\") +\n  theme_light()\n\n\n\n\n\n\n\n\nThis is a decent fit. The coefficient estimates are\n\ncoef(km_fit_1_lm)\n\n(Intercept)       log_t \n  -7.402713    1.223005 \n\n\nso \\(\\mu = -\\frac{-7.403}{1.223} = -6.053\\) and \\(\\sigma = \\frac{1}{1.223} = {0.818}\\).\nCompare two Weibull distributions using the accelerated failure time (AFT) model. This model assumes the survival time for the treatment group is a multiple, \\(e^\\gamma\\), of the control group survival time. The survival distributions in the AFT model are related as \\(S_1(t) = S_0(e^{-\\gamma}t)\\) and the hazards are related by \\(h_1(t) = e^{-\\gamma}h_0(e^{-\\gamma}t)\\). In the case of the Weibull distribution, the relationship is \\(h_1(t) = e^{-\\frac{\\gamma}{\\sigma}}h_0(t)\\). Fit a Weibull model with survreg() (recall KM is fit with survfit()). Return to the original model using the lung data set to compare survival between males and females.\n\ndat &lt;- lung %&gt;% mutate(sex = factor(sex, levels = c(1, 2), labels = c(\"Male\", \"Female\")))\nwb_fit &lt;- survreg(Surv(time, status) ~ sex, data = dat, dist = \"weibull\")\nsummary(wb_fit) \n\n\nCall:\nsurvreg(formula = Surv(time, status) ~ sex, data = dat, dist = \"weibull\")\n              Value Std. Error     z       p\n(Intercept)  5.8842     0.0720 81.76 &lt; 2e-16\nsexFemale    0.3956     0.1276  3.10  0.0019\nLog(scale)  -0.2809     0.0619 -4.54 5.7e-06\n\nScale= 0.755 \n\nWeibull distribution\nLoglik(model)= -1148.7   Loglik(intercept only)= -1153.9\n    Chisq= 10.4 on 1 degrees of freedom, p= 0.0013 \nNumber of Newton-Raphson Iterations: 5 \nn= 228 \n\n\n\\(\\hat{\\gamma} = 0.3956\\), meaning females have longer times until death by a factor of \\(e^{\\hat{\\gamma}} = e^{0.3956} = 1.49\\). The scale parameter estimate is \\(\\hat\\sigma = 0.755\\), so the log proportional hazards is \\(\\hat\\beta = -\\frac{\\hat\\gamma}{\\hat\\sigma} = \\frac{0.3956}{0.755} = 0.524\\).\nThe survival curve estimate is \\(\\hat{S}(t) = e^{-e^{-\\hat\\mu/\\hat\\sigma}t^{1/\\hat\\sigma}}\\), but \\(\\hat\\alpha = 1 / \\hat\\sigma\\).\n\nnew_dat &lt;- expand.grid(\n  sex = levels(dat$sex), \n  survival = seq(.01, .99, by = .01)\n  ) %&gt;%\n  mutate(\n    pred = map2(sex, survival, \n                ~predict(wb_fit, type = \"quantile\", p = 1 - .y, se = TRUE, \n                         newdata = data.frame(sex = .x))),\n    t = map_dbl(pred, ~pluck(.x, \"fit\")),\n    se = map_dbl(pred, ~pluck(.x, \"se.fit\")),\n    ucl = t + 1.96 * se,\n    lcl = t - 1.96 * se\n  )\n\npalette_sex &lt;- c(\"#E7B800\", \"#2E9FDF\")\nnames(palette_sex) &lt;- c(\"Male\", \"Female\")\n\nnew_dat %&gt;%\n  ggplot(aes(y = survival)) +\n  geom_line(aes(x = t, color = sex)) +\n  geom_ribbon(aes(xmin = lcl, xmax = ucl, fill = sex), alpha = 0.2) +\n  scale_color_manual(values = palette_sex) +\n  scale_fill_manual(values = palette_sex) +\n  theme_light()\n\n\n\n\n\n\n\n\nUse predict() to get survival expectations.\n\n# 90% of subjects fail by time \nwb_fit %&gt;% predict(type = \"quantile\", p = .9, newdata = data.frame(sex = levels(dat$sex)))\n##         1         2 \n##  674.4717 1001.7539\n\n\n# Median survival times\npredict(wb_fit, type = \"quantile\", p = 1 - 0.5, newdata = data.frame(sex = levels(dat$sex)))\n\n       1        2 \n272.4383 404.6370 \n\n\nYou can fit other models with the dist = c(\"lognormal\", \"exponential\") parameter.\n\n\n\n\n\n\nZhang, Zhongheng. 2016. “Parametric Regression Model for Survival Data: Weibull Regression Model as an Example.” Annals of Translational Medicine 4 (24). https://atm.amegroups.com/article/view/11446.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Parametric</span>"
    ]
  },
  {
    "objectID": "04-cox.html",
    "href": "04-cox.html",
    "title": "4  Cox Regression",
    "section": "",
    "text": "4.1 Fitting the Model\nThe KM curve with logrank test is a univariate analyses describing the effect of a single categorical factor variable on survival. Parametric and semi-parametric models, on the other hand, are able to describe the effect of multiple covariates. The Cox proportional hazards model is a semi-parametric model. The Cox model expresses the expected hazard, \\(h(t)\\), as an unspecified baseline hazard, \\(h_0(t)\\), multiplied by the exponential of a linear combination of parameters, \\(\\psi = e^{X\\beta}\\).\n\\[h(t) = h_0(t) \\cdot e^{X \\beta} = \\psi h_0(t). \\tag{4.1}\\]\n\\(h_0(t)\\) is unspecified because it cancels out of the model. To see this, consider a hypothetical data set where 6 participants are assumed to fall into one of two hazards: participants 1, 2, and 3 have \\(h_0\\) and 4, 5, and 6 have \\(\\psi h_0\\). Working through the event occurrences, the survival function is the product of each of the failure probabilities. Suppose participant 1 fails first. \\(p_1 = \\frac{h_0(t_1)}{3 \\cdot h_0(t_1) + 3 \\psi \\cdot h_0(t_1)}\\). Suppose participant 2 censors, then participant 4 fails. \\(p_2 = \\frac{\\psi h_0(t_2)}{1 \\cdot h_0(t_2) + 3 \\psi \\cdot h_0(t_2)}\\). Now participant 3 fails. \\(p_3 = \\frac{h_0(t_3)}{1 \\cdot h_0(t_3) + 2 \\psi \\cdot h_0(t_3)}\\). Notice how \\(h_0(t)\\) cancels in each ratio. The partial likelihood is the product of the failure probabilities, \\(L(\\psi) = \\frac{\\psi}{(3 + 3 \\psi)(1 + 3 \\psi)(1 + 2 \\psi)}\\).\nFind the value of \\(\\psi\\) that maximizes \\(L(\\psi)\\). \\(L(\\psi)\\) is difficult to optimize, but its log is easier. \\(l(\\beta) = X \\beta - \\log (3 + 3 e^{X\\beta}) - \\log (1 + 3 e^{X\\beta}) - \\log (1 + 2 e^{X\\beta})\\). A function searches for the \\(\\beta\\) producing the global max. \\(h_0(t)\\), the baseline hazard, is estimated with non-parametric methods.\nThe Cox model is essentially a multiple linear regression of the log hazard on \\(X\\), with the baseline hazard \\(h_0(t)\\) acting as a time-dependent intercept term. The covariates act multiplicatively on the hazard at any point in time. Thus the hazard is proportional to the covariate values (Bradburn 2003). The proportionality of the model comes from the lack of time dependence in the \\(X\\) variables. The ratio of the hazard functions of two individuals is\n\\[\\frac{h_i(t)}{h_j(t)} = \\frac{h_0(t) \\cdot e^{X_i \\beta}}{h_0(t) \\cdot e^{X_j \\beta}} = e^{(\\Delta X)\\beta}.\\]\nThe ratio of person i’s and person j’s hazard at any time t is a function of the difference in the regression variables, not of t. I.e., i’s hazard is a constant proportion of j’s hazard.\nThere are three ways to test the null hypothesis that coefficients have no effect, \\(H_0 : \\beta = 0\\): the Wald test, the score test, and the likelihood ratio test.\nThe Cox proportional hazards model is analogous to the logistic regression model. Rearranging \\(h(t) = h_0(t) \\cdot e^{X \\beta}\\),\n\\[\\ln \\left[ \\frac{h(t)}{h_0(t)} \\right] = X \\beta. \\tag{4.2}\\]\nWhereas logistic regression predicts the log odds of the response, Cox regression predicts the log relative hazard (relative to the unspecified baseline) of the response. \\(\\beta\\) is the change in the log of the relative hazard associated with a one unit change in \\(X\\). Its antilog is the hazard ratio (HR). A positive \\(e^{\\beta_j}\\) means the hazard increases with the covariate.\n\\[\n\\begin{eqnarray}\n(x_b - x_a) \\beta &=& \\ln \\left[ \\frac{h_b(t)}{h_0(t)} \\right] - \\ln \\left[ \\frac{h_a(t)}{h_0(t)} \\right] \\\\\n\\beta &=& \\ln \\left[ \\frac{h_b(t)}{h_a(t)} \\right] \\\\\ne^\\beta &=& \\frac{h_b(t)}{h_a(t)} \\\\\n&=& \\mathrm{HR}\n\\end{eqnarray}\n\\]\nThe Cox proportional hazards model is demonstrated here with a case study using the survival::lung data set used in Chapter 2. The study investigated differences in all-cause mortality between men and women diagnosed with advanced lung cancer. 227 participants aged 39 to 82 were monitored up to three years until time of death. The participants were segmented into three groups according to their ECOG performance score: Asymptomatic, symptomatic but completely Ambulatory, and Bedridden. Participants’ age and gender were captured as controlling covariates. We couldn’t make use of them in the KM analysis, but we can now. Table 4.1 presents the summary statistics of the data set.\nFit a Cox proportional hazards model with survival::coxph().\ncox_fit &lt;- coxph(\n  Surv(time, status == \"died\") ~ sex + ph.ecog + age,\n  data = d_lung\n)\n\nsummary(cox_fit)\n\nCall:\ncoxph(formula = Surv(time, status == \"died\") ~ sex + ph.ecog + \n    age, data = d_lung)\n\n  n= 227, number of events= 164 \n\n                       coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \nsexFemale         -0.551322  0.576188  0.167987 -3.282  0.00103 ** \nph.ecogAmbulatory  0.409461  1.506006  0.199596  2.051  0.04022 *  \nph.ecogBedridden   0.915752  2.498654  0.227042  4.033  5.5e-05 ***\nage                0.011031  1.011092  0.009297  1.186  0.23544    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                  exp(coef) exp(-coef) lower .95 upper .95\nsexFemale            0.5762     1.7355    0.4145    0.8009\nph.ecogAmbulatory    1.5060     0.6640    1.0184    2.2270\nph.ecogBedridden     2.4987     0.4002    1.6012    3.8991\nage                  1.0111     0.9890    0.9928    1.0297\n\nConcordance= 0.637  (se = 0.025 )\nLikelihood ratio test= 30.08  on 4 df,   p=5e-06\nWald test            = 29.77  on 4 df,   p=5e-06\nScore (logrank) test = 30.94  on 4 df,   p=3e-06\nA negative coefficient estimator means the hazard decreases with increasing values of the variable. Females have a log hazard of death equal to coef(cox_fit)[1] = -0.55 of that of males. The exponential is the hazard ratio (HR), the effect-size of the covariate. Being female reduces the hazard by a factor of exp(coef(cox_fit)[1]) = 0.58 (42%). I.e., at any given time, 0.58 times as many females die as males.\nThe last section of the summary object is the three tests for the overall significance of the model. These three methods are asymptotically equivalent. The likelihood ratio test has better behavior for small sample sizes, so it is generally preferred. The p-values for all three tests are significant, indicating that the model is significant (i.e., not all \\(\\beta\\) values are 0). Present the results with gtsummary.\ngtsummary::tbl_regression(cox_fit, exponentiate = TRUE) |&gt;\n  gtsummary::add_glance_source_note()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nHR\n95% CI\np-value\n\n\n\n\nsex\n\n\n\n\n\n\n\n\n    Male\n—\n—\n\n\n\n\n    Female\n0.58\n0.41, 0.80\n0.001\n\n\nph.ecog\n\n\n\n\n\n\n\n\n    Asymptomatic\n—\n—\n\n\n\n\n    Ambulatory\n1.51\n1.02, 2.23\n0.040\n\n\n    Bedridden\n2.50\n1.60, 3.90\n&lt;0.001\n\n\nage\n1.01\n0.99, 1.03\n0.2\n\n\n\nAbbreviations: CI = Confidence Interval, HR = Hazard Ratio\n\n\nn = 227; N events = 164; statistic.log = 30.1; p.value.log = 0.000; statistic.sc = 30.9; p.value.sc = 0.000; statistic.wald = 29.8; p.value.wald = 0.000; statistic.robust = NA; p.value.robust = NA; R² = 0.124; r.squared.max = 0.999; c-index = 0.637; c-index SE = 0.025; Log-likelihood = -729; AIC = 1,467; BIC = 1,479; No. Obs. = 164\nVisualize the fitted Cox model for each risk group. Function survfit() estimates survival at the mean values of covariates by default. That’s usually not useful, so instead pass a data frame with test cases into the newdata argument.\nShow the code\n# Predictions will be for all levels of sex and ph.ecog, but only at the median\n# age.\nnew_dat &lt;- \n  expand.grid(\n    sex = levels(d_lung$sex),\n    ph.ecog = levels(d_lung$ph.ecog),\n    age = median(d_lung$age)\n  ) |&gt; \n  # strata is our key to join back to the fitted values.\n  mutate(strata = as.factor(row_number()))\n\n# Create fitted survival curves at the covariate presets.\nfit_curves &lt;- survfit(cox_fit, newdata = new_dat, data = d_lung)\n\n# `surv_summary()` is like `summary()` except that it includes risk table info,\n# confidence interval attributes, and pivots the strata longer.\nsurv_summary &lt;-\n  surv_summary(fit_curves) |&gt;\n    # The cases are labeled \"strata\", but `survsummary()` doesn't label what the \n    # strata are! Get it from new_dat.\n    inner_join(new_dat, by = \"strata\")\n\n# Now use ggplot() just like normal.\nmedian_line &lt;- \n  surv_summary |&gt;\n  filter(surv &gt;= .5) |&gt;\n  summarize(.by = c(sex, ph.ecog), max_t = max(time))\n\nsurv_summary |&gt;\n  ggplot(aes(x = time, y = surv)) +\n  geom_line(aes(color = ph.ecog)) +\n  geom_ribbon(\n    aes(ymin = lower, ymax = upper, color = ph.ecog, fill = ph.ecog),\n    alpha = 0.4\n  ) +\n  geom_segment(\n    data = median_line,\n    aes(x = 0, xend = max_t, y = .5, yend = .5),\n    linetype = 2, color = \"gray40\"\n  ) +\n  geom_segment(\n    data = median_line,\n    aes(x = max_t, xend = max_t, y = 0, yend = .5, color = ph.ecog),\n    linetype = 2\n  ) +\n  facet_wrap(facets = vars(sex)) +\n  scale_y_continuous(labels = percent_format(1)) +\n  scale_color_manual(values = my_palette$warm) +\n  scale_fill_manual(values = my_palette$warm) +\n  labs(\n    X = \"Time\", y = \"Survival Probability\", color = NULL, fill = NULL,\n    title = \"Cox fitted model\",\n    subtitle = \"Age held at median.\"\n  )\n\n\n\n\n\n\n\n\nFigure 4.1: Fitted survival curves at preset covariate values (age = median).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cox Regression</span>"
    ]
  },
  {
    "objectID": "04-cox.html#assumptions",
    "href": "04-cox.html#assumptions",
    "title": "4  Cox Regression",
    "section": "4.2 Assumptions",
    "text": "4.2 Assumptions\nThe Cox model is valid when censoring is independent of the probability of experiencing the event and there are no omitted variables. Additionally, there are two empircally testable assumptions (Ref1, Ref2).\n\nHazards are proportional. At any time t, all individuals experience a multiple of the same baseline hazard, \\(h_0(t)\\). A violation may occur if, for instance, a treatment loses effectiveness over time.\nLinearity. The log of the hazard is linearly related to each of the covariates.\n\nTest the proportionality assumption with Schoenfeld residuals. Schoenfeld residuals are like normal residuals (\\(y\\) vs \\(\\hat{y}\\)) except that they predict \\(X\\) and compare \\(\\hat{X}\\) to \\(X\\). survival::cox.zph() correlates scaled Schoenfeld residuals with time. If the hazards are time-invariant, the coefficient estimates will be zero. p-values &lt; .05 reject the null hypothesis that the slope is zero. Below, the test is not statistically significant for each of the covariates, and the global test is also not statistically significant, so the proportional hazards assumption holds.\n\n(cox_test_ph &lt;- cox.zph(cox_fit))\n\n        chisq df    p\nsex      2.54  1 0.11\nph.ecog  3.04  2 0.22\nage      0.20  1 0.65\nGLOBAL   5.24  4 0.26\n\n\nPlot the residuals with ggcoxzph(). There should be no systematic trend (the LOESS line is flat). If a covariate violates the PH assumption, consider adding time-dependent interactions or stratifying the model by that variable.\n\nggcoxzph(cox_test_ph) \n\n\n\n\n\n\n\n\nTest the linearity assumption by plotting the Martingale residuals against the continuous covariates. A systematic curve or pattern indicates nonlinearity, so look for random scatter around zero. If you see a U-shape, S-shape, or any consistent deviation, consider transforming the covariate.\n\ncoxph(Surv(time, status == \"died\") ~ age, data = d_lung) |&gt;\n  ggcoxdiagnostics(type = \"martingale\", ox.scale = \"observation.id\")\n\n\n\n\n\n\n\n\nYou can also test for influential observations with deviance residuals. Deviance residuals are a normalized transformation of the Martingale residuals. Deviance residuals should be symmetrically distributed about zero with a standard deviation under 3. Remember that outliers are not necessarily a problem - investigate them.\n\np &lt;- \n  ggcoxdiagnostics(cox_fit, type = \"deviance\", ox.scale = \"observation.id\") +\n  geom_point_interactive(aes(tooltip = glue(\"Obs. {xval}\")))\n\ngirafe(ggobj = p)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cox Regression</span>"
    ]
  },
  {
    "objectID": "04-cox.html#interpreting-results",
    "href": "04-cox.html#interpreting-results",
    "title": "4  Cox Regression",
    "section": "4.3 Interpreting Results",
    "text": "4.3 Interpreting Results\nTable 4.2 shows the effect sizes as hazard ratios with 95% confidence intervals (CI) for each covariate in relation to overall survival. Each factor is assessed through separate univariate Cox regressions. All three factors of the multivariate model show how the factors jointly impact on survival. Being female increased survival, and higher ECOG scores impaired survival. Age was not statistically different from zero at the .05 level. The p-values for the comparisons between each ECOG type are presented, but it is possible to also show an overall likelihood ratio test for the differences between the categories as a whole.\n\nsex_gt &lt;-\n  coxph(Surv(time, status == \"died\") ~ sex, data = d_lung) |&gt;\n  tbl_regression(exponentiate = TRUE)\n\nph.ecog_gt &lt;-\n  coxph(Surv(time, status == \"died\") ~ ph.ecog, data = d_lung) |&gt;\n  tbl_regression(exponentiate = TRUE)\n\nage_gt &lt;-\n  coxph(Surv(time, status == \"died\") ~ age, data = d_lung) |&gt;\n  tbl_regression(exponentiate = TRUE)\n\ncox_uni_gt &lt;- gtsummary::tbl_stack(list(sex_gt, ph.ecog_gt, age_gt))\n\ncox_multi_gt &lt;- tbl_regression(cox_fit, exponentiate = TRUE)\n\ngtsummary::tbl_merge(\n  list(cox_uni_gt, cox_multi_gt),\n  tab_spanner = c(\"Univariate analyses\", \"Multivaraite analysis\")\n)\n\n\n\nTable 4.2: Hazard ratios from Cox PH model for lung dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nUnivariate analyses\n\n\nMultivaraite analysis\n\n\n\nHR\n95% CI\np-value\nHR\n95% CI\np-value\n\n\n\n\nsex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Male\n—\n—\n\n\n—\n—\n\n\n\n\n    Female\n0.59\n0.43, 0.82\n0.002\n0.58\n0.41, 0.80\n0.001\n\n\nph.ecog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Asymptomatic\n—\n—\n\n\n—\n—\n\n\n\n\n    Ambulatory\n1.45\n0.98, 2.13\n0.064\n1.51\n1.02, 2.23\n0.040\n\n\n    Bedridden\n2.54\n1.64, 3.93\n&lt;0.001\n2.50\n1.60, 3.90\n&lt;0.001\n\n\nage\n1.02\n1.00, 1.04\n0.040\n1.01\n0.99, 1.03\n0.2\n\n\n\nAbbreviations: CI = Confidence Interval, HR = Hazard Ratio",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cox Regression</span>"
    ]
  },
  {
    "objectID": "04-cox.html#reporting",
    "href": "04-cox.html#reporting",
    "title": "4  Cox Regression",
    "section": "4.4 Reporting",
    "text": "4.4 Reporting\nThe guidelines for reporting the Kaplan-Meier test are from Laerd’s Kaplan-Meier using SPSS Statistics (Laerd 2015). Report the results like this.\n\n227 Men and women diagnosed with advanced lung cancer aged 39 to 82 were monitored up to three years until time of death. Participants were classified into three groups according to their ECOG performance score: asymptomatic (n = 63), symptomatic but completely ambulatory (n = 113), and bedridden at least part of the day (n = 51). A Cox proportional hazards survival analysis was conducted to compare survival times among the three ECOG performance scores. A proportional hazards test confirmed the proportionality assumption, \\(\\chi^2\\)(4) = 5.2, p = 0.264. Inspection of the deviance residuals confirmed there were no influential data points. The linearity assumption was confirmed by inspection of the Martingale residuals plotted against the continuous covariates. Censored cases were negatively associated with symptom severity, ambulatory, 1.51 (95% CI 1.02, 2.23; p=0.040), and bedridden, 2.50 (95% CI 1.60, 3.90; p&lt;0.001) study groups. Participants that were ambulatory had a hazard ratio (HR) of 1.51 (95% CI 1.02, 2.23; p=0.040) relative to the asymptomatic group. The bedridden group had an HR of 2.50 (95% CI 1.60, 3.90; p&lt;0.001). Sex female was protective against mortality, 0.58 (95% CI 0.41, 0.80; p=0.001).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cox Regression</span>"
    ]
  },
  {
    "objectID": "04-cox.html#landmark-analysis",
    "href": "04-cox.html#landmark-analysis",
    "title": "4  Cox Regression",
    "section": "4.5 Landmark Analysis",
    "text": "4.5 Landmark Analysis\nA landmark analysis measures survival after a milestone period. E.g., the dataset may present survival times after disease onset, but a treatment typically starts after 90 days. In a KM analysis manually adjust the data. In coxph() use the subset argument.\n\ncoxph(\n  Surv(time, status == \"died\") ~ age + sex + ph.ecog, \n  subset = time &gt; 90, \n  data = d_lung\n)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cox Regression</span>"
    ]
  },
  {
    "objectID": "04-cox.html#sec-time-dependent-covariates",
    "href": "04-cox.html#sec-time-dependent-covariates",
    "title": "4  Cox Regression",
    "section": "4.6 Time-Dependent Covariates",
    "text": "4.6 Time-Dependent Covariates\nIf a covariate changes over time, lengthen the underlying data set with tmerge() to create time spans for the covariate values.\nDataset survival::pbc contains baseline and follow-up status for 312 people with primary biliary cirrhosis (PBC). survival::pbcseq contains their periodic lab results.\nStatuses are 0 (censored), 1 (liver transplant), died (status = 2). Person 1 died at 400 days. Person 2 was still alive at 4,500 days.\n\npbc_0 &lt;- survival::pbc |&gt; filter(id &lt;= 312) |&gt; select(c(id:sex))\n\nhead(pbc_0)\n\n  id time status trt      age sex\n1  1  400      2   1 58.76523   f\n2  2 4500      0   1 56.44627   f\n3  3 1012      2   1 70.07255   m\n4  4 1925      2   1 54.74059   f\n5  5 1504      1   2 38.10541   f\n6  6 2503      2   2 66.25873   f\n\n\nThe first step is to merge the base dataset with itself to define the studied event status, creating tstart, tstop, and death.\n\npbc_1 &lt;- tmerge(pbc_0, pbc_0, id = id, death = event(time, status))\n\nhead(pbc_1)\n\n  id time status trt      age sex tstart tstop death\n1  1  400      2   1 58.76523   f      0   400     2\n2  2 4500      0   1 56.44627   f      0  4500     0\n3  3 1012      2   1 70.07255   m      0  1012     2\n4  4 1925      2   1 54.74059   f      0  1925     2\n5  5 1504      1   2 38.10541   f      0  1504     1\n6  6 2503      2   2 66.25873   f      0  2503     2\n\n\nThe second source of data is the time-dependent covariate data, survival::pbcseq. Person 1 had lab work done on day 0 and 192. Person 2 had lab work on 9 occasions.\n\nseq_0 &lt;- survival::pbcseq |&gt; select(id, day, bili, protime)\n\nseq_0 |&gt; filter(id == 1)\n\n  id day bili protime\n1  1   0 14.5    12.2\n2  1 192 21.3    11.2\n\nseq_0 |&gt; filter(id == 2)\n\n  id  day bili protime\n1  2    0  1.1    10.6\n2  2  182  0.8    11.0\n3  2  365  1.0    11.6\n4  2  768  1.9    10.6\n5  2 1790  2.6    11.3\n6  2 2151  3.6    11.5\n7  2 2515  4.2    11.5\n8  2 2882  3.6    11.5\n9  2 3226  4.6    11.5\n\n\nMerge the two datasets together, creating time-dependent covariates for two lab markers, bili and protime.\n\nPerson 1 had their second lab word done on day 192, then died at day 400. Their bili and protime are as of time tstart. Note that age is not time dependent - it describes the person at the time of entering the study.\nPerson 2 was still alive as of the time the data was collected, 4500 days from entering the study. Their last lab work was at day 3226.\n\n\nmdl_dat &lt;-\n  tmerge(\n    pbc_1,\n    seq_0,\n    id = id,\n    bili = tdc(day, bili),\n    protime = tdc(day, protime)\n  )\n\nmdl_dat |&gt; filter(id == 1)\n\n  id time status trt      age sex tstart tstop death bili protime\n1  1  400      2   1 58.76523   f      0   192     0 14.5    12.2\n2  1  400      2   1 58.76523   f    192   400     2 21.3    11.2\n\nmdl_dat |&gt; filter(id == 2)\n\n  id time status trt      age sex tstart tstop death bili protime\n1  2 4500      0   1 56.44627   f      0   182     0  1.1    10.6\n2  2 4500      0   1 56.44627   f    182   365     0  0.8    11.0\n3  2 4500      0   1 56.44627   f    365   768     0  1.0    11.6\n4  2 4500      0   1 56.44627   f    768  1790     0  1.9    10.6\n5  2 4500      0   1 56.44627   f   1790  2151     0  2.6    11.3\n6  2 4500      0   1 56.44627   f   2151  2515     0  3.6    11.5\n7  2 4500      0   1 56.44627   f   2515  2882     0  4.2    11.5\n8  2 4500      0   1 56.44627   f   2882  3226     0  3.6    11.5\n9  2 4500      0   1 56.44627   f   3226  4500     0  4.6    11.5\n\n\nFit the model as usual with coxph(). There are three possible events, so explicitly look for death == 2 (died), treating the other outcomes as censoring.\n\ncox_fit &lt;- coxph(\n  Surv(tstart, tstop, death==2) ~ log(bili) + log(protime) + trt,\n  mdl_dat\n)\n\ntbl_regression(cox_fit)\n\n\n\n\n\n\n\nCharacteristic\nlog(HR)\n95% CI\np-value\n\n\n\n\nlog(bili)\n1.2\n1.1, 1.4\n&lt;0.001\n\n\nlog(protime)\n4.0\n3.1, 4.8\n&lt;0.001\n\n\ntrt\n-0.07\n-0.43, 0.30\n0.7\n\n\n\nAbbreviations: CI = Confidence Interval, HR = Hazard Ratio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBradburn, Clark, M. J. 2003. “Survival Analysis Part II: Multivariate Data Analysis–an Introduction to Concepts and Methods.” British Journal of Cancer 89 (3). https://doi.org/10.1038/sj.bjc.6601119.\n\n\nLaerd. 2015. Statistical Tutorials and Software Guides. https://statistics.laerd.com/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Cox Regression</span>"
    ]
  },
  {
    "objectID": "05-discrete-time.html",
    "href": "05-discrete-time.html",
    "title": "5  Discrete-Time",
    "section": "",
    "text": "Cox extended the proportional hazards model to discrete times using logistic regression. Times are discrete when the events they mark refer to an interval rather than an instant (e.g., grade when dropped out of school). Discrete-time survival models are applied to a person-period data set to predict the hazard of experiencing the failure event during the period intervals. Suresh (2022) demonstrates how these models are constructed.\nThe semi-parametric Cox proportional hazards model relies on the assumption of proportional hazards, a faulty assumption if time-varying covariate effects are not modeled, or there is other unobserved heterogeneity.\nRecall that for continuous times the Cox proportional hazards model fits \\(h(t) = h_0(t) \\cdot e^{X\\beta}\\), and taking the log and rearranging describes a linear relationship between the log relative hazard and the predictor variables, \\(\\ln \\left[ \\frac{h(t)}{h_0(t)} \\right] = X\\beta\\). The antilog of \\(\\beta\\) is a hazard ratio (relative risk).\nFit a discrete-time model to an expanded data set that has one record for each subject and relevant time interval, and a binary variable representing the status. E.g., if event times in the data set occur in the range [16, 24] and individual i has the event at t = 20, then i’s record would expand to five rows with t = [16, 20] and status = [0, 0, 0, 0, 1].\nThe discrete-time model fits either a) the logit of the hazard at period t, \\(\\ln \\left[ \\frac{h(t)}{1 - h(t)} \\right] = \\alpha + X \\beta\\), or the complementary log-log, \\(\\ln (- \\ln (1 - h(t))) = \\alpha + X \\beta\\).1 \\(\\alpha = \\mathrm{logit}\\hspace{1mm}h_0(t)\\) is the logit of the baseline hazard. The model treats time as discrete by introducing one parameter \\(\\alpha_j\\) for the \\(j\\) possible event times in the data set (that’s why you expand the data set). The model fit is regular logistic regression.\nWhereas the antilog of \\(\\beta\\) in the continuous model is the hazard ratio, in the logit model it is the hazard odds ratio. The Cox and logit models converge as \\(t \\rightarrow 0\\) because \\(\\log(h(t) \\sim \\log \\frac{h(t)}{1 - h(t)}\\) as \\(t \\rightarrow 0\\). In the the complementary log-log model, the antilog of \\(\\beta\\) is the hazard ratio, just as in Cox. Specify the link function in glm with family = binomial(link = \"cloglog\").",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete-Time</span>"
    ]
  },
  {
    "objectID": "05-discrete-time.html#footnotes",
    "href": "05-discrete-time.html#footnotes",
    "title": "5  Discrete-Time",
    "section": "",
    "text": "This formulation is derived from the relationship between the survival function to a baseline survival, \\(S(t) = S_0(t)^{\\exp{Xb}}\\). See German Rodriguez’s course notes.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete-Time</span>"
    ]
  },
  {
    "objectID": "06-ml.html",
    "href": "06-ml.html",
    "title": "6  Machine Learning",
    "section": "",
    "text": "6.1 Random forest\nMachine learning (ML) algorithms model non-linear and complex covariate relationships with iterative optimization algorithms that minimize prediction error. The primary goal of ML is prediction. The downside is that it is often difficult to interpret covariate effects and interactions.\nRandom forests are an ensemble of tree-based learners that are built using bootstrap samples of the training data and average the predictions from the individuals trees. In constructing the trees, a random subset of features is selected for evaluating the split criterion at each node. This leads to de-correlated individual trees that can improve predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "06-ml.html#boosting",
    "href": "06-ml.html#boosting",
    "title": "6  Machine Learning",
    "section": "6.2 Boosting",
    "text": "6.2 Boosting\nBoosting are an ensemble of base learners that are constructed sequentially and are progressively reweighted to increase emphasis on observations with wrong predictions and high errors. Thus, the subsequent learners are more likely to correctly classify these mis-classified observations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "06-ml.html#support-vector-machines",
    "href": "06-ml.html#support-vector-machines",
    "title": "6  Machine Learning",
    "section": "6.3 Support vector machines",
    "text": "6.3 Support vector machines\nSupport vector machines (SVMs) use a kernel function to map input features into high-dimensional feature spaces where classification (survival) can be described by a hyperplane.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "06-ml.html#penalized-regression",
    "href": "06-ml.html#penalized-regression",
    "title": "6  Machine Learning",
    "section": "6.4 Penalized regression",
    "text": "6.4 Penalized regression\nPenalized regression provides a mathematical solution to applying regression methods to correlated features by using an ℓ2 penalty term (ridge). Additionally, can encourage sparsity by using an ℓ1 penalty (LASSO) to avoid overfitting and perform variable selection. A weighted combination of ℓ1 and ℓ2 penalties can be used to do both (elastic net).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "06-ml.html#artificial-neural-networks",
    "href": "06-ml.html#artificial-neural-networks",
    "title": "6  Machine Learning",
    "section": "6.5 Artificial neural networks",
    "text": "6.5 Artificial neural networks\nArtificial neural networks are comprised of node layers starting with input layer representing the data features, that feeds into one or more hidden layers, and ends with an output layer that presents the final prediction.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "07-case-study.html",
    "href": "07-case-study.html",
    "title": "7  Case Study",
    "section": "",
    "text": "7.1 Cox\nThe guidelines for reporting the Kaplan-Meier test are from Laerd’s Kaplan-Meier using SPSS Statistics (Laerd 2015). The data is from survival::lung.\nA study investigated differences in all-cause mortality between men and women diagnosed with advanced lung cancer. 227 participants aged 39 to 82 were monitored up to three years until time of death. The participants were segmented into three groups according to their ECOG performance score: asymptomatic, symptomatic but completely ambulatory, and bedridden at least part of the day. Participant age was captured as a controlling covariate.\nThe Cox proportional hazards model describes the effect of explanatory variables on survival and can include controlling variables. Add age as a controlling variable.\ncox_fit &lt;- coxph(Surv(time, status) ~ ph.ecog + age, data = d_lung)\n(cox_tbl &lt;- cox_fit %&gt;% gtsummary::tbl_regression(exponentiate = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nHR\n95% CI\np-value\n\n\n\n\nph.ecog\n\n\n\n\n\n\n\n\n    Asymptomatic\n—\n—\n\n\n\n\n    Ambulatory\n1.43\n0.97, 2.11\n0.072\n\n\n    Bedridden\n2.39\n1.52, 3.74\n&lt;0.001\n\n\nage\n1.01\n0.99, 1.03\n0.2\n\n\n\nAbbreviations: CI = Confidence Interval, HR = Hazard Ratio\nUsing a Cox proportional hazards regression analysis, we find the association between ECOG and survival time statistically significant with a Bedridden estimate of 2.39 (95% CI 1.52, 3.74; p&lt;0.001) relative to Asymptomatic. The hazard ratio for Ambulatory was not statistically significant, 1.43 (95% CI 0.97, 2.11; p=0.072).\nsurvfit(cox_fit, newdata = list(ph.ecog = levels(d_lung$ph.ecog),\n                                             age = rep(median(d_lung$age), 3)), data = d_lung) %&gt;%\n  surv_summary() %&gt;%\n  ggsurvplot_df(\n    fun = \"pct\",\n    # linetype = \"strata\", # Change line type by groups\n    # conf.int = TRUE,\n    surv.median.line = \"hv\", # median horizontal and vertical ref lines\n    ggtheme = theme_light(),\n    palette = c(\"goldenrod\", \"sienna\", \"tomato\"),\n    title = \"Fitted Cox model at Median Participant Age.\",\n    legend.title = \"\",\n    legend.labs = levels(d_lung$ph.ecog)\n  )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study</span>"
    ]
  },
  {
    "objectID": "07-case-study.html#discrete-time",
    "href": "07-case-study.html#discrete-time",
    "title": "7  Case Study",
    "section": "7.2 Discrete Time",
    "text": "7.2 Discrete Time\nTo fit a discrete time model, first create a discrete time data set, on record per relevant time period. You could do this by hand with expand.grid()\n\ncs_glm_dat &lt;- expand.grid(\n  time = seq(min(d_lung$time), max(d_lung$time), 1),\n  patient_id = unique(d_lung$patient_id)\n) %&gt;%\n  inner_join(d_lung, by = \"patient_id\") %&gt;%\n  filter(time.x &lt;= time.y) %&gt;%\n  mutate(status = if_else(time.x &lt; time.y, 0, status - 1)) %&gt;%\n  rename(time = time.x)\n\nBut discSurv::dataLong does this for you.\n\ncs_glm_dat &lt;- d_lung %&gt;% \n  mutate(status = status - 1) %&gt;%\n  discSurv::dataLong(\"time\", \"status\", timeAsFactor = FALSE)\n\nFor example, patient 1 died at time 306, so they have 306 rows in the new data set, with y = 0 for all but the last row.\n\ncs_glm_dat %&gt;% filter(obj == 1) %&gt;% tail()\n\n      obj timeInt y inst time status age sex    ph.ecog ph.karno pat.karno\n1.300   1     301 0    3  306      1  74   1 Ambulatory       90       100\n1.301   1     302 0    3  306      1  74   1 Ambulatory       90       100\n1.302   1     303 0    3  306      1  74   1 Ambulatory       90       100\n1.303   1     304 0    3  306      1  74   1 Ambulatory       90       100\n1.304   1     305 0    3  306      1  74   1 Ambulatory       90       100\n1.305   1     306 1    3  306      1  74   1 Ambulatory       90       100\n      meal.cal wt.loss patient_id\n1.300     1175      NA          1\n1.301     1175      NA          1\n1.302     1175      NA          1\n1.303     1175      NA          1\n1.304     1175      NA          1\n1.305     1175      NA          1\n\n\nglm() fits either the log(-log(1-hazard)) or log(hazard / (1-hazard)) as a function of time, controlling for ph.ecog and age. The two links are similar here1.\n\ncs_glm_dat %&gt;%\n  group_by(ph.ecog, timeInt) %&gt;%\n  summarise(.groups = \"drop\", died = sum(y), at_risk = n()) %&gt;%\n  mutate(\n    hazard = died / at_risk,\n    logit_hazard = log(hazard / (1 - hazard)),\n    cloglog_hazard = log(-log(1 - hazard))\n  ) %&gt;%\n  pivot_longer(cols = c(logit_hazard, cloglog_hazard)) %&gt;%\n  filter(!hazard %in% c(0, 1)) %&gt;%\n  ggplot(aes(x = timeInt, y = value, color = ph.ecog)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = \"y ~ x\") +\n  facet_wrap(facets = vars(name)) +\n  theme_light()\n\n\n\n\n\n\n\n\n\nlogit_fit &lt;- glm(\n  y ~ timeInt + ph.ecog + age, \n  data = cs_glm_dat, \n  family = binomial(link = \"logit\")\n)\n(logit_tbl &lt;- logit_fit %&gt;% gtsummary::tbl_regression(exponentiate = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\ntimeInt\n1.00\n1.00, 1.00\n&lt;0.001\n\n\nph.ecog\n\n\n\n\n\n\n\n\n    Asymptomatic\n—\n—\n\n\n\n\n    Ambulatory\n1.42\n0.97, 2.11\n0.081\n\n\n    Bedridden\n2.38\n1.52, 3.76\n&lt;0.001\n\n\nage\n1.01\n0.99, 1.03\n0.3\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\nUsing a discrete time logistic regression analysis with logit link function, we find the association between ECOG and survival time statistically significant with a Bedridden estimate of 2.38 (95% CI 1.52, 3.76; p&lt;0.001) relative to Asymptomatic. The hazard odds ratio for Ambulatory was not statistically significant, 1.42 (95% CI 0.97, 2.11; p=0.081).\n\ncloglog_fit &lt;- glm(\n  y ~ timeInt + ph.ecog + age, \n  data = cs_glm_dat, \n  family = binomial(link = \"cloglog\")\n)\n(cloglog_tbl &lt;- cloglog_fit %&gt;% gtsummary::tbl_regression(exponentiate = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nHR\n95% CI\np-value\n\n\n\n\ntimeInt\n1.00\n1.00, 1.00\n&lt;0.001\n\n\nph.ecog\n\n\n\n\n\n\n\n\n    Asymptomatic\n—\n—\n\n\n\n\n    Ambulatory\n1.41\n0.97, 2.11\n0.081\n\n\n    Bedridden\n2.38\n1.52, 3.75\n&lt;0.001\n\n\nage\n1.01\n0.99, 1.03\n0.3\n\n\n\nAbbreviations: CI = Confidence Interval, HR = Hazard Ratio\n\n\n\n\n\n\n\n\nUsing a discrete time logistic regression analysis with complementary log-log link function, we find the association between ECOG and survival time statistically significant with a Bedridden estimate of 2.38 (95% CI 1.52, 3.75; p&lt;0.001) relative to Asymptomatic. The hazard ratio for Ambulatory was not statistically significant, 1.41 (95% CI 0.97, 2.11; p=0.081).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study</span>"
    ]
  },
  {
    "objectID": "07-case-study.html#reporting",
    "href": "07-case-study.html#reporting",
    "title": "7  Case Study",
    "section": "7.3 Reporting",
    "text": "7.3 Reporting\nThe reporting below uses the Cox model rather than the discrete time model.\n\nParticipants were randomly assigned to three different interventions in order to quit smoking: a hypnotherapy programme (n = 50), wearing nicotine patches (n = 50) and using e-cigarettes (n = 50). Kaplan-Meier survival analysis (Kaplan & Meier, 1958) was conducted to compare the three different interventions for their effectiveness in preventing smoking resumption. A similar percentage of censored cases was present in the hypnotherapy (12.0%), nicotine patch (14.0%) and e-cigarette (14.0%) intervention groups and the pattern of censoring was similar. Participants that underwent the hypnotherapy programme had a median time to smoking resumption of 69.0 (95% CI, 45.2 to 92.8) days. This was longer than the groups receiving nicotine patches or e-cigarettes, which had identical median times to smoking resumption of 9.0 (95% CI, 6.6 to 11.4) days and 9.0 (95% CI, 7.1 to 10.9) days, respectively. A log rank test was conducted to determine if there were differences in the survival distributions for the different types of intervention. The survival distributions for the three interventions were statistically significantly different, χ2(2) = 25.818, p &lt; .0005. Pairwise log rank comparisons were conducted to determine which intervention groups had different survival distributions. A Bonferroni correction was made with statistical significance accepted at the p &lt; .0167 level. There was a statistically significant difference in survival distributions for the hypnotherapy vs nicotine patch intervention, χ2(1) = 11.035, p = .001, and hypnotherapy vs e-cigarette intervention, χ2(1) = 29.003, p &lt; .0005. However, the survival distributions for the e-cigarette and nicotine patch interventions were not statistically significantly different, χ2(1) = 1.541, p = .214.\n\n\n\n\n\n\n\nLaerd. 2015. Statistical Tutorials and Software Guides. https://statistics.laerd.com/.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study</span>"
    ]
  },
  {
    "objectID": "07-case-study.html#footnotes",
    "href": "07-case-study.html#footnotes",
    "title": "7  Case Study",
    "section": "",
    "text": "Full discussion on Rens van de Schoot.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Case Study</span>"
    ]
  },
  {
    "objectID": "08-references.html",
    "href": "08-references.html",
    "title": "References",
    "section": "",
    "text": "Bradburn, Clark, M. J. 2003. “Survival Analysis Part II:\nMultivariate Data Analysis–an Introduction to Concepts and\nMethods.” British Journal of Cancer 89 (3). https://doi.org/10.1038/sj.bjc.6601119.\n\n\nClark, Bradburn, T. G. 2003. “Survival Analysis Part i: Basic\nConcepts and First Analyses.” British Journal of Cancer\n89 (2). https://doi.org/10.1038/sj.bjc.6601118.\n\n\nLaerd. 2015. Statistical Tutorials and Software Guides. https://statistics.laerd.com/.\n\n\nSuresh, Severn, K. 2022. “Survival Prediction Models: An\nIntroduction to Discrete-Time Modeling.” BMC Med Res\nMethodol 22 (207). https://doi.org/10.1186/s12874-022-01679-6.\n\n\nZhang, Zhongheng. 2016. “Parametric Regression Model for Survival\nData: Weibull Regression Model as an Example.” Annals of\nTranslational Medicine 4 (24). https://atm.amegroups.com/article/view/11446.",
    "crumbs": [
      "References"
    ]
  }
]